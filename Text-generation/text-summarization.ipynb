{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressive Summary Fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import heapq\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "punctuation = punctuation+\"\\n\"+\"“\" + '”' +\"–\"\n",
    "stopWords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy library text summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_spacy(text,percentage=0.4):\n",
    "    doc = nlp(text)\n",
    "    word_freq = Counter(\n",
    "     word.text.lower() for word in doc \n",
    "        if word.text.lower() not in STOP_WORDS \n",
    "        if word.text.lower() not in punctuation \n",
    "    )\n",
    "    max_freq = max(word_freq.values())\n",
    "    word_freq = {k : v/max_freq for k,v in word_freq.items()}\n",
    "    sentence = [sent for sent in doc.sents]\n",
    "    sent_score = {}\n",
    "    for sent in sentence:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_freq:\n",
    "                if sent not in sent_score:\n",
    "                    sent_score[sent] = word_freq[word.text.lower()]\n",
    "                else:\n",
    "                    sent_score[sent] += word_freq[word.text.lower()]\n",
    "    summary = \"\".join(list(map(lambda x:x.text.strip().replace(\"\\n\",\"\") ,\n",
    "              nlargest(int(len(sent_score)*percentage),# getting 40% summary\n",
    "                 sent_score,    key=sent_score.get))))\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK text summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_nltk(text,top_largest=3):\n",
    "    word_frequencies = {}\n",
    "\n",
    "    for word in nltk.word_tokenize(text):  \n",
    "        if word not in stopWords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():  \n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "    sentence_list = nltk.sent_tokenize(text)\n",
    "    sentence_scores = {}  \n",
    "    for sent in sentence_list:  \n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "\n",
    "    summary_sentences = heapq.nlargest(top_largest, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    summary = ' '.join(summary_sentences).strip().replace(\"\\n\",\"\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Sematice analysis text summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(document: str):\n",
    "    doc = TextBlob(document)\n",
    "    corpus_sentence = doc.sentences\n",
    "    documents_list = []\n",
    "    for line in corpus_sentence:\n",
    "        text = line.strip()\n",
    "        if len(text) < 1:\n",
    "            continue\n",
    "        documents_list.append(str(text))\n",
    "    print(\"Total Number of Documents:\",len(documents_list))\n",
    "    return documents_list\n",
    "\n",
    "\n",
    "def preprocess_data(document_list :List[str])-> List[str]:\n",
    "    # intialize regex tokenizer\n",
    "    tokenizer =  RegexpTokenizer(r\"\\w+\")\n",
    "    # stop words\n",
    "    en_stop = set(stopwords.words(\"english\"))\n",
    "    # intialsing portstemmer\n",
    "    portstemer = PorterStemmer()\n",
    "\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for text in document_list:\n",
    "        # tokenizing data with only words\n",
    "        tokens  = tokenizer.tokenize(text.lower())\n",
    "        \n",
    "        # remove stop words from tokens\n",
    "        tokens = [word for word in tokens if word not in en_stop]\n",
    "        \n",
    "        # stem tokens\n",
    "        tokens = [portstemer.stem(word) for word in tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(tokens)\n",
    "    return texts\n",
    "\n",
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input: clean document\n",
    "    Output : dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculation term dictionary of corpus\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # document term matrix\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    return dictionary, doc_term_matrix\n",
    "\n",
    "\n",
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix = prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    #print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel\n",
    "\n",
    "\n",
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "def takenext(elem):\n",
    "        \"\"\"\n",
    "        sort\n",
    "        \"\"\"\n",
    "        return elem[1]\n",
    "\n",
    "def selectTopSent(summSize, numTopics, vecsSort):\n",
    "    topSentences = []\n",
    "    sent_no = []\n",
    "    sentIndexes = set()\n",
    "    sCount = 0\n",
    "    for i in range(summSize):\n",
    "        for j in range(numTopics):\n",
    "            vecs = vecsSort[j]\n",
    "            si = vecs[i][0]\n",
    "            if si not in sentIndexes:\n",
    "                sent_no.append(si)\n",
    "                sCount +=1\n",
    "                topSentences.append(vecs[i])\n",
    "                sentIndexes.add(si)\n",
    "                if sCount == summSize:\n",
    "                    sent_no\n",
    "        return sent_no    \n",
    "\n",
    "    \n",
    "def summary_latent_semantic_analysis(text,number_of_topics=2,words=50):\n",
    "    # sentence document\n",
    "    document_list = load_data(text)\n",
    "    # cleanup data\n",
    "    clean_text = preprocess_data(document_list)\n",
    "    # corpus_dict and document term matrix\n",
    "    dict1, doc_term_matrix = prepare_corpus(clean_text)\n",
    "    # using genensime lsa model\n",
    "    model = create_gensim_lsa_model(clean_text, number_of_topics, words)\n",
    "    corpus_lsi = model[doc_term_matrix]\n",
    "    #sort each vector by score\n",
    "    vecsSort = list(map(lambda i: list(), range(number_of_topics)))\n",
    "    # adding vecssort with value\n",
    "    for i,docv in enumerate(corpus_lsi):\n",
    "        for sc in docv:\n",
    "            isent = (i, abs(sc[1]))\n",
    "            vecsSort[sc[0]].append(isent)\n",
    "    # sorting vector according to value\n",
    "    vecsSort = list(map(lambda x: sorted(x,key=takenext,reverse=True), vecsSort))\n",
    "    topSentences = selectTopSent(8, 2, vecsSort)\n",
    "    topSentences.sort()\n",
    "    summary = []\n",
    "    doc = []\n",
    "    for idx ,sentence in enumerate(document_list):\n",
    "        if idx in topSentences:\n",
    "            summary.append(sentence)   \n",
    "    summary = \" \".join(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"\"\"Marathi (English: /məˈrɑːti/;[5] मराठी, Marāṭhī, Marathi: [məˈɾaːʈʰiː] (About this soundlisten)) is an Indo-Aryan language spoken predominantly by around 83 million Marathi people of Maharashtra, India. It is the official language and co-official language in the Maharashtra and Goa states of Western India, respectively and is one of the 22 scheduled languages of India. With 83 million speakers as 2011, Marathi ranks 10th in the list of languages with most native speakers in the world. Marathi has the third largest number of native speakers in India, after Hindi and Bengali.[6] The language has some of the oldest literature of all modern Indian languages, dating from around 600 AD.[7] The major dialects of Marathi are Standard Marathi and the Varhadi dialect.[8] Koli, Agri and Malvani Konkani have been heavily influenced by Marathi varieties.Marathi distinguishes inclusive and exclusive forms of 'we' and possesses a three-way gender system that features the neuter in addition to the masculine and the feminine.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Marathi (English: /məˈrɑːti/;[5] मराठी, Marāṭhī, Marathi: [məˈɾaːʈʰiː] (About this soundlisten)) is an Indo-Aryan language spoken predominantly by around 83 million Marathi people of Maharashtra, India.The major dialects of Marathi are Standard Marathi and the Varhadi dialect.[8] Koli, Agri and Malvani Konkani have been heavily influenced by Marathi varieties.'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_spacy(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Marathi (English: /məˈrɑːti/;[5] मराठी, Marāṭhī, Marathi: [məˈɾaːʈʰiː] (About this soundlisten)) is an Indo-Aryan language spoken predominantly by around 83 million Marathi people of Maharashtra, India. [6] The language has some of the oldest literature of all modern Indian languages, dating from around 600 AD. With 83 million speakers as 2011, Marathi ranks 10th in the list of languages with most native speakers in the world.'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_nltk(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documents: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Marathi (English: /məˈrɑːti/;[5] मराठी, Marāṭhī, Marathi: [məˈɾaːʈʰiː] (About this soundlisten)) is an Indo-Aryan language spoken predominantly by around 83 million Marathi people of Maharashtra, India. It is the official language and co-official language in the Maharashtra and Goa states of Western India, respectively and is one of the 22 scheduled languages of India.'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_latent_semantic_analysis(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('env-nlp-1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "21f57ecafef1f012a4c16c1d85d21eb2a4ad7f03b85fa78165186b61e626e569"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
