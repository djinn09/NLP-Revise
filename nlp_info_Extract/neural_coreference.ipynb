{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_coreference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5g9yIbmhQkC"
      },
      "source": [
        "# Hugging and AllenNlp combining coreference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIL0jLusTL8A",
        "outputId": "de42e121-1ba9-48eb-d74c-6ff271d7fa51"
      },
      "source": [
        "!pip install allennlp==1.4.1 --quiet\n",
        "!pip install --pre allennlp-models==1.4.0 --quiet\n",
        "!pip install spacy==2.1.0 --quiet\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install neuralcoref --no-binary neuralcoref\n",
        "!pip install nltk==3.6.5\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 2.6 MB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: neuralcoref in /usr/local/lib/python3.7/dist-packages (4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (1.19.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (1.20.21)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.23.0)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.6)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.62.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.24.0,>=1.23.21 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref) (1.23.21)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref) (0.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.21->boto3->neuralcoref) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.21->boto3->neuralcoref) (1.15.0)\n",
            "Requirement already satisfied: nltk==3.6.5 in /usr/local/lib/python3.7/dist-packages (3.6.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (4.62.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (2021.11.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LYiny12_Pqn",
        "outputId": "192ec2db-aafe-4f34-d405-ff9af9a36ce5"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YuvU80YwC2ae",
        "outputId": "2247e9dc-94a3-4c84-93d8-650bccbb960e"
      },
      "source": [
        "neuralcoref.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4.0.0'"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXK6NEN7PkzI"
      },
      "source": [
        "import spacy\n",
        "import neuralcoref"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_VuDkg1P8k-",
        "outputId": "da676a0f-515a-46b5-bb72-18b34c3b6ccb"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "neuralcoref.add_to_pipe(nlp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7efd5617f410>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMuzeOYCVOxs"
      },
      "source": [
        "# text = \"Eva and Martha didn't want their friend Jenny to feel lonely so they invited her to the party in Las Vegas.\"\n",
        "# text = \"Shivaji Bhonsale I (Marathi pronunciation c.19 February 1630 – 3 April 1680[5]), also referred to as Chhatrapati Shivaji, was an Indian ruler and a member of the Bhonsle Maratha clan. Shivaji carved out an enclave from the declining Adilshahi sultanate of Bijapur that formed the genesis of the Maratha Empire. In 1674, he was formally crowned the Chhatrapati of his realm at Raigad\"\n",
        "text = '''Every Tuesday and Friday, Recode’s Kara Swisher and NYU Professor Scott Galloway offer sharp, unfiltered insights into the biggest stories in tech, business, and politics. They make bold predictions, pick winners and losers, and bicker and banter like no one else. Kara is out welcoming the newest member of the Pivot family! Scott is joined by co-host Stephanie Ruhle to talk about The Great Resignation, inflation, J&J’s split, and Steve Bannon’s indictment. Also, Elon is still bullying senators on Twitter, and Beto is officially running for Governor of Texas. Plus, Scott chats with Friend of Pivot, Founder and CEO of Boom Supersonic, Blake Scholl about supersonic air travel.'''\n",
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JRLT0N3VPKN",
        "outputId": "9d71da51-129c-4cbc-be05-bd466f6cabcd"
      },
      "source": [
        "# it has two clusters \n",
        "clusters = doc._.coref_clusters\n",
        "clusters\n",
        "# [a machine learning algorithm: [a machine learning algorithm, it],\n",
        "#  the training set: [the training set, the training set]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Every Tuesday and Friday, Recode’s Kara Swisher and NYU Professor Scott Galloway: [Every Tuesday and Friday, Recode’s Kara Swisher and NYU Professor Scott Galloway, They],\n",
              " Scott: [Scott, Scott]]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksaQ1_WAXKOa"
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_gCsOJKZXVz"
      },
      "source": [
        "# model_url = 'https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz' # Old model\n",
        "model_url = \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\"# new model\n",
        "predictor = Predictor.from_path(model_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp1kics3Zjsu"
      },
      "source": [
        "prediction = predictor.predict(document=text)\n",
        "coref_res = predictor.coref_resolved(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9SVFrqEZn97",
        "outputId": "d28d7ace-4cc7-40c9-8578-6004d97ce30e"
      },
      "source": [
        "print(' '.join(prediction['document']))\n",
        "print(coref_res)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every Tuesday and Friday , Recode ’s Kara Swisher and NYU Professor Scott Galloway offer sharp , unfiltered insights into the biggest stories in tech , business , and politics . They make bold predictions , pick winners and losers , and bicker and banter like no one else . Kara is out welcoming the newest member of the Pivot family ! Scott is joined by co - host Stephanie Ruhle to talk about The Great Resignation , inflation , J&J ’s split , and Steve Bannon ’s indictment . Also , Elon is still bullying senators on Twitter , and Beto is officially running for Governor of Texas . Plus , Scott chats with Friend of Pivot , Founder and CEO of Boom Supersonic , Blake Scholl about supersonic air travel .\n",
            "Every Tuesday and Friday, Recode’s Kara Swisher and NYU Professor Scott Galloway offer sharp, unfiltered insights into the biggest stories in tech, business, and politics. Recode’s Kara Swisher and NYU Professor Scott Galloway make bold predictions, pick winners and losers, and bicker and banter like no one else. Recode’s Kara Swisher is out welcoming the newest member of the Pivot family! NYU Professor Scott Galloway is joined by co-host Stephanie Ruhle to talk about The Great Resignation, inflation, J&J’s split, and Steve Bannon’s indictment. Also, Elon is still bullying senators on Twitter, and Beto is officially running for Governor of Texas. Plus, NYU Professor Scott Galloway chats with Friend of the Pivot family, Founder and CEO of Boom Supersonic, Blake Scholl about supersonic air travel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk3czcBUZtgw"
      },
      "source": [
        "def get_span_words(span, document):\n",
        "    return ' '.join(document[span[0]:span[1]+1])\n",
        "\n",
        "def print_clusters(prediction):\n",
        "    document, clusters = prediction['document'], prediction['clusters']\n",
        "    for cluster in clusters:\n",
        "        print(get_span_words(cluster[0], document) + ': ', end='')\n",
        "        print(f\"[{'; '.join([get_span_words(span, document) for span in cluster])}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQPoRs9JZuCX"
      },
      "source": [
        "# print_clusters(prediction)\n",
        "# old model\n",
        "# deciding: [deciding; it]\n",
        "# your training set: [your training set; the training set; the training set]\n",
        "# using: [using; it]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GNzjw3HZvvF",
        "outputId": "f7254aa4-92f9-4b96-e640-8eecc9821a59"
      },
      "source": [
        "print_clusters(prediction)\n",
        "# new model\n",
        "# deciding: [deciding; it]\n",
        "# your training set: [your training set; the training set; the training set]\n",
        "# using: [using; it]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recode ’s Kara Swisher and NYU Professor Scott Galloway: [Recode ’s Kara Swisher and NYU Professor Scott Galloway; They]\n",
            "Recode ’s Kara Swisher: [Recode ’s Kara Swisher; Kara]\n",
            "NYU Professor Scott Galloway: [NYU Professor Scott Galloway; Scott; Scott]\n",
            "the Pivot family: [the Pivot family; Pivot]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toEqDM1uq_nW"
      },
      "source": [
        "#Models intersection strategies (ensemble)\n",
        "We use two models: **AllenNLP** and **Huggingface**. In multiple tests AllenNLP turned out much better - it has better precision and recall (on Google GAP dataset), and finds much more clusters at all. That's why we decide to take AllenNLP answers as a ground truth.\n",
        "However AllenNLP also makes mistakes - it has about 8% of false positives which we would like to minimize. That's why we propose **several intersections of AllenNLP and Huggingface** outputs (an ensemble) to modify the results and gain more confidence about the final clusters.\n",
        "\n",
        "We propose **3** intersection strategies:\n",
        "\n",
        "\n",
        "*   strict - we take only those clusters that are identical both in AllenNLP and Huggingface (intersection of clusters)\n",
        "*   partial - we take all of the spans that are identical both in AllenNLP and Huggingface (intersection of spans/mentions)\n",
        "*   fuzzy - we take all of the spans that are the same but also we find spans that overlap (are relating to the same entity but are composed of different number of tokens) and choose the shorter one\n",
        "*   List item\n",
        "\n",
        "\n",
        "##Example\n",
        "\n",
        "**Text**\n",
        "\n",
        "In 1311 it was settled on Peter and Lucy for life with remainder to William Everard and his wife Beatrice. Peter had died by 1329 but Lucy lived until 1337 and she was succeeded by William Everard who died in 1343. William's son, Sir Edmund Everard inherited and maintained ownership jointly with his wife Felice until he died in 1370.\n",
        "\n",
        "**AllenNLP clusters**\n",
        "\n",
        "William Everard --> William Everard; his; William Everard who died in 1343; William's\n",
        "\n",
        "Peter --> Peter; Peter\n",
        "\n",
        "Lucy --> Lucy; Lucy; she\n",
        "\n",
        "William's son, Sir Edmund Everard --> William's son, Sir Edmund Everard; his; he\n",
        "\n",
        "**Huggingface clusters**\n",
        "\n",
        "William Everard --> William Everard; his; William Everard; Sir Edmund Everard; his\n",
        "\n",
        "Peter --> Peter; Peter; he\n",
        "\n",
        "Lucy --> Lucy; Lucy; she\n",
        "\n",
        "his wife Beatrice --> his wife Beatrice; his wife Felice\n",
        "\n",
        "\n",
        "**Strategies**\n",
        "\n",
        "\n",
        "1.   **Strict**\n",
        "\n",
        "        Lucy --> Lucy; Lucy; she\n",
        "2.   **Partial**\n",
        "        William Everard --> William Everard; his\n",
        "\n",
        "        Peter --> Peter; Peter\n",
        "\n",
        "        Lucy --> Lucy; Lucy; she\n",
        "\n",
        "\n",
        "3.   **Fuzzy**\n",
        "\n",
        "      William Everard --> William Everard; his; William Everard\n",
        "\n",
        "      Peter --> Peter; Peter\n",
        "\n",
        "      Lucy --> Lucy; Lucy; she\n",
        "\n",
        "      Sir Edmund Everard --> Sir Edmund Everard; his\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DIeniIOae6K"
      },
      "source": [
        "from copy import deepcopy\n",
        "import neuralcoref\n",
        "import en_core_web_sm\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from os import environ\n",
        "from warnings import warn\n",
        "from typing import Dict, List\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "\n",
        "class IntersectionStrategy(ABC):\n",
        "\n",
        "    def __init__(self, allen_model, hugging_model):\n",
        "        self.allen_clusters = []\n",
        "        self.hugging_clusters = []\n",
        "        self.allen_model = allen_model\n",
        "        self.hugging_model = hugging_model\n",
        "        self.document = []\n",
        "        self.doc = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_intersected_clusters(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def get_span_noun_indices(doc: Doc, cluster: List[List[int]]):\n",
        "        spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
        "        spans_pos = [[token.pos_ for token in span] for span in spans]\n",
        "        span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
        "            if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
        "        return span_noun_indices\n",
        "\n",
        "    @staticmethod\n",
        "    def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):\n",
        "        head_idx = noun_indices[0]\n",
        "        head_start, head_end = cluster[head_idx]\n",
        "        head_span = doc[head_start:head_end+1]\n",
        "        return head_span, [head_start, head_end]\n",
        "\n",
        "    @staticmethod\n",
        "    def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):\n",
        "        return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\n",
        "\n",
        "    def coref_resolved_improved(self, doc: Doc, clusters: List[List[List[int]]]):\n",
        "        resolved = [tok.text_with_ws for tok in doc]\n",
        "        all_spans = [span for cluster in clusters for span in cluster]  # flattened list of all spans\n",
        "\n",
        "        for cluster in clusters:\n",
        "            noun_indices = self.get_span_noun_indices(doc, cluster)\n",
        "            if noun_indices:\n",
        "                mention_span, mention = self.get_cluster_head(doc, cluster, noun_indices)\n",
        "\n",
        "                for coref in cluster:\n",
        "                    if coref != mention and not self.is_containing_other_spans(coref, all_spans):\n",
        "                        final_token = doc[coref[1]]\n",
        "                        if final_token.tag_ in [\"PRP$\", \"POS\"]:\n",
        "                            resolved[coref[0]] = mention_span.text + \"'s\" + final_token.whitespace_\n",
        "                        else:\n",
        "                            resolved[coref[0]] = mention_span.text + final_token.whitespace_\n",
        "\n",
        "                        for i in range(coref[0] + 1, coref[1] + 1):\n",
        "                            resolved[i] = \"\"\n",
        "\n",
        "        return \"\".join(resolved)\n",
        "\n",
        "    def clusters(self, text):\n",
        "        self.acquire_models_clusters(text)\n",
        "        return self.get_intersected_clusters()\n",
        "\n",
        "    def resolve_coreferences(self, text: str):\n",
        "        clusters = self.clusters(text)\n",
        "        resolved_text = self.coref_resolved_improved(self.doc, clusters)\n",
        "        return resolved_text\n",
        "\n",
        "    def acquire_models_clusters(self, text: str):\n",
        "        allen_prediction = self.allen_model.predict(text)\n",
        "        self.allen_clusters = allen_prediction['clusters']\n",
        "        self.document = allen_prediction['document']\n",
        "        self.doc = self.hugging_model(text)\n",
        "        hugging_clusters = self._transform_huggingface_answer_to_allen_list_of_clusters()\n",
        "        self.hugging_clusters = hugging_clusters\n",
        "\n",
        "    def _transform_huggingface_answer_to_allen_list_of_clusters(self):\n",
        "        list_of_clusters = []\n",
        "        for cluster in self.doc._.coref_clusters:\n",
        "            list_of_clusters.append([])\n",
        "            for span in cluster:\n",
        "                list_of_clusters[-1].append([span[0].i, span[-1].i])\n",
        "        return list_of_clusters\n",
        "\n",
        "\n",
        "class PartialIntersectionStrategy(IntersectionStrategy):\n",
        "\n",
        "    def get_intersected_clusters(self):\n",
        "        intersected_clusters = []\n",
        "        for allen_cluster in self.allen_clusters:\n",
        "            intersected_cluster = []\n",
        "            for hugging_cluster in self.hugging_clusters:\n",
        "                allen_set = set(tuple([tuple(span) for span in allen_cluster]))\n",
        "                hugging_set = set(tuple([tuple(span) for span in hugging_cluster]))\n",
        "                intersect = sorted([list(el) for el in allen_set.intersection(hugging_set)])\n",
        "                if len(intersect) > 1:\n",
        "                    intersected_cluster += intersect\n",
        "            if intersected_cluster:\n",
        "                intersected_clusters.append(intersected_cluster)\n",
        "        return intersected_clusters\n",
        "\n",
        "class FuzzyIntersectionStrategy(PartialIntersectionStrategy):\n",
        "    \"\"\" Is treated as a PartialIntersectionStrategy, yet first must map AllenNLP spans and Huggingface spans. \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def flatten_cluster(list_of_clusters):\n",
        "        return [span for cluster in list_of_clusters for span in cluster]\n",
        "\n",
        "    def _check_whether_spans_are_within_range(self, allen_span, hugging_span):\n",
        "        allen_range = range(allen_span[0], allen_span[1]+1)\n",
        "        hugging_range = range(hugging_span[0], hugging_span[1]+1)\n",
        "        allen_within = allen_span[0] in hugging_range and allen_span[1] in hugging_range\n",
        "        hugging_within = hugging_span[0] in allen_range and hugging_span[1] in allen_range\n",
        "        return allen_within or hugging_within\n",
        "\n",
        "    def _add_span_to_list_dict(self, allen_span, hugging_span):\n",
        "        if (allen_span[1]-allen_span[0] > hugging_span[1]-hugging_span[0]):\n",
        "            self._add_element(allen_span, hugging_span)\n",
        "        else:\n",
        "            self._add_element(hugging_span, allen_span)\n",
        "\n",
        "    def _add_element(self, key_span, val_span):\n",
        "        if tuple(key_span) in self.swap_dict_list.keys():\n",
        "            self.swap_dict_list[tuple(key_span)].append(tuple(val_span))\n",
        "        else:\n",
        "            self.swap_dict_list[tuple(key_span)] = [tuple(val_span)]\n",
        "\n",
        "    def _filter_out_swap_dict(self):\n",
        "        swap_dict = {}\n",
        "        for key, vals in self.swap_dict_list.items():\n",
        "            if self.swap_dict_list[key] != vals[0]:\n",
        "                swap_dict[key] = sorted(vals, key=lambda x: x[1]-x[0], reverse=True)[0]\n",
        "        return swap_dict\n",
        "\n",
        "    def _swap_mapped_spans(self, list_of_clusters, model_dict):\n",
        "        for cluster_idx, cluster in enumerate(list_of_clusters):\n",
        "            for span_idx, span in enumerate(cluster):\n",
        "                if tuple(span) in model_dict.keys():\n",
        "                    list_of_clusters[cluster_idx][span_idx] = list(model_dict[tuple(span)])\n",
        "        return list_of_clusters\n",
        "\n",
        "    def get_mapped_spans_in_lists_of_clusters(self):\n",
        "        self.swap_dict_list = {}\n",
        "        for allen_span in self.flatten_cluster(self.allen_clusters):\n",
        "            for hugging_span in self.flatten_cluster(self.hugging_clusters):\n",
        "                if self._check_whether_spans_are_within_range(allen_span, hugging_span):\n",
        "                    self._add_span_to_list_dict(allen_span, hugging_span)\n",
        "        swap_dict = self._filter_out_swap_dict()\n",
        "\n",
        "        allen_clusters_mapped = self._swap_mapped_spans(deepcopy(self.allen_clusters), swap_dict)\n",
        "        hugging_clusters_mapped = self._swap_mapped_spans(deepcopy(self.hugging_clusters), swap_dict)\n",
        "        return allen_clusters_mapped, hugging_clusters_mapped\n",
        "\n",
        "    def get_intersected_clusters(self):\n",
        "        allen_clusters_mapped, hugging_clusters_mapped = self.get_mapped_spans_in_lists_of_clusters()\n",
        "        self.allen_clusters = allen_clusters_mapped\n",
        "        self.hugging_clusters = hugging_clusters_mapped\n",
        "        return super().get_intersected_clusters()\n",
        "\n",
        "\n",
        "\n",
        "class StrictIntersectionStrategy(IntersectionStrategy):\n",
        "\n",
        "    def get_intersected_clusters(self):\n",
        "        intersected_clusters = []\n",
        "        for allen_cluster in self.allen_clusters:\n",
        "            for hugging_cluster in self.hugging_clusters:\n",
        "                if allen_cluster == hugging_cluster:\n",
        "                    intersected_clusters.append(allen_cluster)\n",
        "        return intersected_clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6MzhRrOstQH",
        "outputId": "8082f367-7017-4a5e-a76b-ef7643a59536"
      },
      "source": [
        "print(\"~~~ AllenNLP clusters ~~~\")\n",
        "print_clusters(prediction)\n",
        "print(\"\\n~~~ Huggingface clusters ~~~\")\n",
        "for cluster in doc._.coref_clusters:\n",
        "    print(cluster)\n",
        "\n",
        "# ~~~ AllenNLP clusters ~~~\n",
        "# deciding: [deciding; it]\n",
        "# your training set: [your training set; the training set; the training set]\n",
        "# using: [using; it]\n",
        "\n",
        "# ~~~ Huggingface clusters ~~~\n",
        "# a machine learning algorithm: [a machine learning algorithm, it]\n",
        "# the training set: [the training set, the training set]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~ AllenNLP clusters ~~~\n",
            "Recode ’s Kara Swisher and NYU Professor Scott Galloway: [Recode ’s Kara Swisher and NYU Professor Scott Galloway; They]\n",
            "Recode ’s Kara Swisher: [Recode ’s Kara Swisher; Kara]\n",
            "NYU Professor Scott Galloway: [NYU Professor Scott Galloway; Scott; Scott]\n",
            "the Pivot family: [the Pivot family; Pivot]\n",
            "\n",
            "~~~ Huggingface clusters ~~~\n",
            "Every Tuesday and Friday, Recode’s Kara Swisher and NYU Professor Scott Galloway: [Every Tuesday and Friday, Recode’s Kara Swisher and NYU Professor Scott Galloway, They]\n",
            "Scott: [Scott, Scott]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmt4OZSIstX5"
      },
      "source": [
        "strict = StrictIntersectionStrategy(predictor, nlp)\n",
        "partial = PartialIntersectionStrategy(predictor, nlp)\n",
        "fuzzy = FuzzyIntersectionStrategy(predictor, nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZZU_6WDstaj"
      },
      "source": [
        "\n",
        "def get_cluster_head_idx(doc, cluster):\n",
        "    noun_indices = IntersectionStrategy.get_span_noun_indices(doc, cluster)\n",
        "    return noun_indices[0] if noun_indices else 0\n",
        "\n",
        "\n",
        "def print_clusters(doc, clusters):\n",
        "    def get_span_words(span, allen_document):\n",
        "        return ' '.join(allen_document[span[0]:span[1]+1])\n",
        "\n",
        "    allen_document, clusters = [t.text for t in doc], clusters\n",
        "    # new_clusters = []\n",
        "    for cluster in clusters:\n",
        "        cluster_head_idx = get_cluster_head_idx(doc, cluster)\n",
        "        if cluster_head_idx >= 0:\n",
        "            cluster_head = cluster[cluster_head_idx]\n",
        "            # key = get_span_words(cluster_head, allen_document)\n",
        "            print(get_span_words(cluster_head, allen_document) + ' - ', end='')\n",
        "            print('[', end='')\n",
        "            value = []\n",
        "            for i, span in enumerate(cluster):\n",
        "                print(get_span_words(span, allen_document) + (\"; \" if i+1 < len(cluster) else \"\"), end='')\n",
        "                value.append(get_span_words(span, allen_document))\n",
        "\n",
        "            print(']')\n",
        "            # new_clusters.append((key,value))\n",
        "    # return new_clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OvNXl88stdW",
        "outputId": "a4cda42f-5a4c-4775-e419-01e045435699"
      },
      "source": [
        "for intersection_strategy in [strict, partial, fuzzy]:\n",
        "    print(f'\\n~~~ {intersection_strategy.__class__.__name__} clusters ~~~')\n",
        "    print(print_clusters(doc, intersection_strategy.clusters(text)))\n",
        "\n",
        "# ~~~ StrictIntersectionStrategy clusters ~~~\n",
        "\n",
        "# ~~~ PartialIntersectionStrategy clusters ~~~\n",
        "# the training set - [the training set; the training set]\n",
        "\n",
        "# ~~~ FuzzyIntersectionStrategy clusters ~~~\n",
        "# the training set - [the training set; the training set]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "~~~ StrictIntersectionStrategy clusters ~~~\n",
            "[]\n",
            "\n",
            "~~~ PartialIntersectionStrategy clusters ~~~\n",
            "Scott - [Scott; Scott]\n",
            "[('Scott', ['Scott', 'Scott'])]\n",
            "\n",
            "~~~ FuzzyIntersectionStrategy clusters ~~~\n",
            "Recode ’s Kara Swisher and NYU Professor Scott Galloway - [Recode ’s Kara Swisher and NYU Professor Scott Galloway; They]\n",
            "Scott - [Scott; Scott]\n",
            "[('Recode ’s Kara Swisher and NYU Professor Scott Galloway', ['Recode ’s Kara Swisher and NYU Professor Scott Galloway', 'They']), ('Scott', ['Scott', 'Scott'])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfN-1ZIVubsX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l8ztUWRubxO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKAfcbC7mlDG"
      },
      "source": [
        "##The Problem\n",
        "AllenNLP coreference resolution models seems to find better clusters - numerous clusters that are usually more accurate than the ones found by Huggingface NeuralCoref model. However, the biggest problem lies in the next step - the step of replacing found mentions with the most meaningfull spans from each clusters (that we call the \"heads\"). We've found a couple of easy-to-fix problems which seem to lead to errors most often. Our ideas can be summed up as:\n",
        "\n",
        "\n",
        "*   not resolving coreferences in the clusters that doesn't contain any noun phrases (usually it comes down to the clusters composed only of pronouns),\n",
        "*  chosing the head of the cluster which is a noun phrase (isn't a pronoun),\n",
        "*   resolving only the inner span in the case of nested coreferent mentions.\n",
        "\n",
        "Original AllenNLP impelemntation of the replace_corefs method\n",
        "We extract the main \"logic\" into the separate function that will be used in our every method as we leave the core of AllenNLP's logic untouched. So as for now, we will compare our solutions to the original_replace_corefs method implemented in AllenNLP coref.py (we've just copied it here explicitly in order to compare with the improved method we propose).\n",
        "\n",
        "##Original AllenNLP impelemntation of the replace_corefs method\n",
        "\n",
        "We extract the main \"logic\" into the separate function that will be used in our every method as we leave the core of AllenNLP's logic untouched. So as for now, we will compare our solutions to the `original_replace_corefs` method implemented in AllenNLP `coref.py` (we've just copied it here explicitly in order to compare with the improved method we propose)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAo3dKlka2Bv"
      },
      "source": [
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "def core_logic_part(document: Doc, coref: List[int], resolved: List[str], mention_span: Span):\n",
        "    final_token = document[coref[1]]\n",
        "    if final_token.tag_ in [\"PRP$\", \"POS\"]:\n",
        "        resolved[coref[0]] = mention_span.text + \"'s\" + final_token.whitespace_\n",
        "    else:\n",
        "        resolved[coref[0]] = mention_span.text + final_token.whitespace_\n",
        "    for i in range(coref[0] + 1, coref[1] + 1):\n",
        "        resolved[i] = \"\"\n",
        "    return resolved\n",
        "\n",
        "\n",
        "def get_span_noun_indices(doc: Doc, cluster: List[List[int]]) -> List[int]:\n",
        "    spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
        "    spans_pos = [[token.pos_ for token in span] for span in spans]\n",
        "    span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
        "        if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
        "    return span_noun_indices\n",
        "\n",
        "def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):\n",
        "    return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\n",
        "\n",
        "def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):\n",
        "    head_idx = noun_indices[0]\n",
        "    head_start, head_end = cluster[head_idx]\n",
        "    head_span = doc[head_start:head_end+1]\n",
        "    return head_span, [head_start, head_end]\n",
        "\n",
        "def improved_replace_corefs(document, clusters):\n",
        "    \"\"\"\n",
        "    Nested coreferent mentions\n",
        "    \"\"\"\n",
        "    resolved = list(tok.text_with_ws for tok in document)\n",
        "    all_spans = [span for cluster in clusters for span in cluster]  # flattened list of all spans\n",
        "\n",
        "    for cluster in clusters:\n",
        "        noun_indices = get_span_noun_indices(document, cluster)\n",
        "\n",
        "        if noun_indices:\n",
        "            mention_span, mention = get_cluster_head(document, cluster, noun_indices)\n",
        "\n",
        "            for coref in cluster:\n",
        "                if coref != mention and not is_containing_other_spans(coref, all_spans):\n",
        "                    core_logic_part(document, coref, resolved, mention_span)\n",
        "\n",
        "    return \"\".join(resolved)\n",
        "\n",
        "\n",
        "# def improved_replace_corefs(document, clusters):\n",
        "#     \"\"\" Corefecnt not head  \"\"\"\n",
        "#     resolved = list(tok.text_with_ws for tok in document)\n",
        "\n",
        "#     for cluster in clusters:\n",
        "#         noun_indices = get_span_noun_indices(document, cluster)\n",
        "\n",
        "#         if noun_indices:\n",
        "#             mention_span, mention = get_cluster_head(document, cluster, noun_indices)\n",
        "\n",
        "#             for coref in cluster:\n",
        "#                 if coref != mention:  # we don't replace the head itself\n",
        "#                     core_logic_part(document, coref, resolved, mention_span)\n",
        "\n",
        "#     return \"\".join(resolved)\n",
        "\n",
        "\"\"\"\n",
        "Improvements\n",
        "Redundant clusters - lack of a meaningfull mention that could become the head\n",
        "We completely ignore (we don't resove them at all) the clusters that doesn't contain any noun phrase.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def original_replace_corefs(document: Doc, clusters: List[List[List[int]]]) -> str:\n",
        "    resolved = list(tok.text_with_ws for tok in document)\n",
        "\n",
        "    for cluster in clusters:\n",
        "        mention_start, mention_end = cluster[0][0], cluster[0][1] + 1\n",
        "        mention_span = document[mention_start:mention_end]\n",
        "\n",
        "        for coref in cluster[1:]:\n",
        "            core_logic_part(document, coref, resolved, mention_span)\n",
        "\n",
        "    return \"\".join(resolved)\n",
        "\n",
        "\n",
        "\n",
        "def print_comparison(resolved_original_text, resolved_improved_text):\n",
        "    print(f\"~~~ AllenNLP original replace_corefs ~~~\\n{resolved_original_text}\")\n",
        "    print(f\"\\n~~~ Our improved replace_corefs ~~~\\n{resolved_improved_text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwC6_H8wnuc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71a9436-b212-4717-b8e3-43ef96ccef59"
      },
      "source": [
        "t = \"We want to take our code and create a game. Let's remind ourselves how to do that.\" #'\"He is a great actor!\", he said about John Travolta.'\n",
        "clusters = predictor.predict(t)['clusters']\n",
        "doc = nlp(t)\n",
        "print(clusters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0, 0], [4, 4], [12, 12], [14, 14]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVhdbyb4numk",
        "outputId": "cb1c83ce-f749-45ea-d795-ef2f05b17fa4"
      },
      "source": [
        "print_comparison(original_replace_corefs(doc, clusters), improved_replace_corefs(doc, clusters))\n",
        "# ~~~ AllenNLP original replace_corefs ~~~\n",
        "# We want to take We's code and create a game. LetWe remind We how to do that.\n",
        "\n",
        "# ~~~ Our improved replace_corefs ~~~\n",
        "# We want to take our code and create a game. Let's remind ourselves how to do that.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~ AllenNLP original replace_corefs ~~~\n",
            "We want to take We's code and create a game. LetWe remind We how to do that.\n",
            "\n",
            "~~~ Our improved replace_corefs ~~~\n",
            "We want to take our code and create a game. Let's remind ourselves how to do that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu7JKofynupk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}